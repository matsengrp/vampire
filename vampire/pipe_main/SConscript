import json
import os
from os.path import join
import re

import numpy as np  # noqa

import nestly
import nestly.scons as ns
import SCons.Script as sc

import common
import tcr_vae

from common import cluster_execution_string

sc.Import('env')
localenv = env.Clone()  # noqa


def check_mode():
    if localenv['mode'] not in ['mini', 'default']:
        raise Exception(f"Unknown mode '{localenv['mode']}'")


def apply_mode(l):
    """
    Default mode runs everything, and mini mode just runs a single element from the list.
    """
    check_mode()
    if localenv['mode'] == 'mini':
        return [l[0]]
    else:
        return l


def default_params_by_mode():
    """
    Mini mode doesn't train for long.
    """
    params = tcr_vae.TCRVAE.default_params()

    if localenv['mode'] == 'mini':
        params['pretrains'] = 2
        params['warmup_period'] = 3
        params['epochs'] = 10

    return params


def base_dict():
    """
    The dictionary that will be shared by all the nests.

    nseqs: the number of sequences generated by the various programs, and for
    which we evaluate Pvae for on the real data.

    train_size: the number of sequences to take for training.

    max_q: the level for truncating q_{lvj} in the thymic Q calculation.
    """

    d = {'nseqs': 10000,
         'train_size': 100000,
         'max_q': 100,
         }

    if localenv['mode'] == 'mini':
        d['nseqs'] = 100

    # Slurp up all key-value pairs in the JSON file into the base_dict.
    with open(localenv['data']) as fp:
        for k, v in json.load(fp).items():
            d[k] = v

    return d


def numerical_nest_add(nest_name, number_list):
    """
    Add an nest for a list of non-negative numbers, with nice zero-padded
    directory names.
    """
    nest.add(nest_name, apply_mode(number_list), label_func=common.zero_pad_list_func(number_list))


# ### Nests and targets ###

nest = ns.SConsWrap(nestly.Nest(base_dict=base_dict()), alias_environment=localenv)

# Nest: the data set choice, named via data name prepended with `_output_`.
nest.add('data_label', [localenv['data']], label_func=lambda p: '_output_' + common.strip_dirpath_extn(p))

# The test_set_agg allows us to process the test sets once in the things that don't depend on the VAE.
nest.add_aggregate('test_set_agg', list)
# The test_set_info_agg gathers information about the processed test sets so we can get at them later.
nest.add_aggregate('test_set_info_agg', dict)
# The summarized_agg gathers everything we want to summarize and then stack at the end.
nest.add_aggregate('summarized_agg', list)
# The merged gathers merged per-sequence information on the test set.
nest.add_aggregate('merged_agg', list)
summarized_agg_names = []
# nest.add_aggregate('loss_regression_agg', list)


@nest.add_target_with_env(localenv)
def sconscript(env, outdir, c):
    """
    Copy SConscript file into output directory.
    """
    return env.Command(
        join(outdir, 'SConscript'),
        'SConscript',
        'cp $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv)
def json_data(env, outdir, c):
    """
    Copy JSON data file into output directory.
    """
    return env.Command(
        join(outdir, os.path.basename(localenv['data'])),
        localenv['data'],
        'cp $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv)
def vampire_version(env, outdir, c):
    """
    Echo vampire version to file.
    """
    return env.Command(
        join(outdir, 'vampire-version.txt'),
        [],
        'python -c "import vampire; print(vampire.__version__)" > $TARGET')[0]


# Nest: initial processing of the test sets.
nest.add('test_data', lambda c: c['test_paths'], label_func=common.strip_dirpath_extn)


@nest.add_target_with_env(localenv)
def test_head(env, outdir, c):
    """
    Run the preprocess_adaptive.py script on the test data, which incorporates
    downsampling.
    """
    test_head_path = join(outdir, common.strip_dirpath_extn(c['test_data'])+'.head.csv')
    # Remove a pesky and meaningless leading `./` that was making trouble later.
    c['test_set_agg'].append(re.sub('^\./', '', test_head_path))
    return env.Command(
        test_head_path,
        c['test_data'],
        f"python3 preprocess_adaptive.py --sample {c['nseqs']} $SOURCE $TARGET")[0]


@nest.add_target_with_env(localenv)
def test_pgen(env, outdir, c):
    test_pgen_path = join(outdir, 'test-head.pgen.tsv')
    c['test_set_info_agg'][(str(c['test_head']), 'test_pgen')] = test_pgen_path
    return env.Command(
        test_pgen_path,
        c['test_head'],
        cluster_execution_string('adaptive-pgen.sh {sources} {targets}', localenv, 0))[0]


nest.pop('test_data')


# Nest: the training data.
# By using a nest we retain the option of having multiple training sets later.
nest.add('train_data', lambda c: [c['train_tsv_path']], label_func=common.strip_dirpath_extn)
summarized_agg_names.append('train_data')


# Nest: training set size
# numerical_nest_add('train_size', np.floor(common.logspace(5000, 500000, 6)))
# summarized_agg_names.append('train_size')


@nest.add_target_with_env(localenv)
def preprocess(env, outdir, c):
    """
    Run the preprocess_adaptive.py script on the training/validation data.
    """
    in_path = c['train_data']
    # We take enough for training and validation_head, with nseqs to spare.
    n_to_sample = c['train_size'] + 2 * c['nseqs']
    return env.Command(
        join(outdir, 'train.processed.csv'),
        in_path,
        f'python3 preprocess_adaptive.py --sample {n_to_sample} $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv, 'split')
@ns.name_targets
def split(env, outdir, c):
    """
    Split the training data into training and validation.
    """
    in_path = c['preprocess']
    return 'training', 'validation', env.Command(
        [join(outdir, 'training.csv'), join(outdir, 'validation.csv')],
        in_path,
        f"python3 util.py split --train-size {c['train_size']} $SOURCE $TARGETS")


@nest.add_target_with_env(localenv)
def training_olga_tsv(env, outdir, c):
    """
    Get Pgen for the training data.
    """
    return env.Command(
        join(outdir, 'training.olga.tsv'),
        c['split']['training'],
        'gene-name-conversion adaptive2olga $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv)
def training_head(env, outdir, c):
    return env.Command(
        common.strip_extn(c['split']['training'])+'.head.csv',
        c['split']['training'],
        f"head -n {c['nseqs']+1} $SOURCE > $TARGET")[0]


@nest.add_target_with_env(localenv)
def training_head_pgen(env, outdir, c):
    """
    Get Pgen for some of the training data.
    """
    return env.Command(
        join(outdir, 'training.head.pgen.tsv'),
        c['training_head'],
        'adaptive-pgen.sh $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv)
def validation_head(env, outdir, c):
    return env.Command(
        common.strip_extn(c['split']['validation'])+'.head.csv',
        c['split']['validation'],
        f"head -n {c['nseqs']+1} $SOURCE > $TARGET")[0]


@nest.add_target_with_env(localenv)
def q_csv(env, outdir, c):
    """
    Calculate values for the thymic Q.
    """
    return env.Command(
        join(outdir, 'q.tsv'),
        c['training_olga_tsv'],
        f"python3 thymic_Q.py q --max-q {c['max_q']} data/thymic-Q/model-lvj-frequency.csv.bz2 $SOURCE $TARGET")[0]


@nest.add_target_with_env(localenv)
def training_ppost(env, outdir, c):
    return env.Command(
        join(outdir, common.strip_dirpath_extn(c['training_head_pgen'])+'.ppost.csv'),
        [c['q_csv'], c['training_head_pgen']],
        'python3 thymic_Q.py ppost $SOURCES $TARGET')[0]


@nest.add_target_with_env(localenv)
def validation_pgen(env, outdir, c):
    return env.Command(
        join(outdir, common.strip_dirpath_extn(c['validation_head'])+'.pgen.csv'),
        c['validation_head'],
        cluster_execution_string('adaptive-pgen.sh {sources} {targets}', localenv, 0))[0]


@nest.add_target_with_env(localenv)
def validation_ppost(env, outdir, c):
    return env.Command(
        common.strip_extn(c['validation_pgen'])+'.ppost.csv',
        [c['q_csv'], c['validation_pgen']],
        'python3 thymic_Q.py ppost $SOURCES $TARGET')[0]


# Nest: loop back over the test sets so we can calculate things for OLGA.
nest.add('test_set', lambda c: c['test_set_agg'], label_func=common.strip_dirpath_extn)


@nest.add_target_with_env(localenv)
def test_ppost(env, outdir, c):
    c['test_pgen'] = c['test_set_info_agg'][(c['test_set'], 'test_pgen')]
    test_ppost_path = join(outdir, 'ppost.csv'),
    c['test_set_info_agg'][(c['test_set'], 'test_ppost')] = test_ppost_path
    return env.Command(
        test_ppost_path,
        [c['q_csv'], c['test_pgen']],
        'python3 thymic_Q.py ppost $SOURCES $TARGET')[0]


nest.pop('test_set')

# Nest: the dimension of the latent space.
# numerical_nest_add('warmup_period', [5*i for i in range(0,6)])
# summarized_agg_names.append('warmup_period')

# Nest: the dimension of the latent space.
# numerical_nest_add('latent_dim', [30, 35, 40])
# summarized_agg_names.append('latent_dim')

# Nest: the number of dense nodes.
# numerical_nest_add('dense_nodes', [50, 75, 100, 125, 150])
# summarized_agg_names.append('dense_nodes')

# Nest: the amino acid embedding dimension.
# numerical_nest_add('aa_embedding_dim', [10, 15, 20, 21])
# summarized_agg_names.append('aa_embedding_dim')

# Nest: the V gene embedding dimension.
# numerical_nest_add('v_gene_embedding_dim', [20, 30, 40])
# summarized_agg_names.append('v_gene_embedding_dim')

# Nest: the strength of the KL component of the VAE loss.
# numerical_nest_add('beta', np.linspace(0.625, 1, 7))
numerical_nest_add('beta', [0.75])
summarized_agg_names.append('beta')

# Nest: monitor for EarlyStopping.
# nest.add('stopping_monitor', ['loss', 'val_loss'])
# summarized_agg_names.append('stopping_monitor')

# Nest: EarlyStopping patience.
# numerical_nest_add('patience', [5*x for x in range(7)])
# summarized_agg_names.append('patience')


@nest.add_target_with_env(localenv)
def olga_generated_tsv(env, outdir, c):
    """
    Generate sequences using OLGA + thymic Q.
    """
    return env.Command(
        join(outdir, 'olga-generated.tsv'),
        c['q_csv'],
        f"python3 thymic_Q.py sample --proposal-size {10*c['nseqs']} "
        f"--max-iter 10000 {c['nseqs']} $SOURCE $TARGET")[0]


@nest.add_target_with_env(localenv)
def olga_generated(env, outdir, c):
    """
    Convert OLGA TSV to an Adaptive CSV.
    """
    return env.Command(
        join(outdir, 'olga-generated.csv'),
        c['olga_generated_tsv'],
        'python3 gene_name_conversion.py olga2adaptive $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv)
def olga_generated_pgen(env, outdir, c):
    return env.Command(
        join(outdir, 'olga-generated.pgen.tsv'),
        c['olga_generated'],
        cluster_execution_string('adaptive-pgen.sh {sources} {targets}', localenv, 0))[0]


@nest.add_target_with_env(localenv)
def olga_generated_ppost(env, outdir, c):
    return env.Command(
        join(outdir, 'olga-generated.ppost.csv'),
        [c['q_csv'], c['olga_generated_pgen']],
        'python3 thymic_Q.py ppost $SOURCES $TARGET')[0]


# Nest: the model.
# This needs to appear in the innermost nest so that we can pop it and then do some OLGA work.
# In order to keep things happy when merging with OLGA results, don't comment
# this out if you are only interested in one model. Instead, make a
# single-model nest.
# nest.add('model', apply_mode([common.strip_dirpath_extn(m) for m in glob.glob('../models/*.py')]))
nest.add('model', ['basic', 'count_match'])
summarized_agg_names.append('model')


@nest.add_target_with_env(localenv)
def model_params(env, outdir, c):
    """
    Write out a file with parameters from which we can build our VAE.
    """

    # Copy over any relevant parameters from c into the params dictionary.
    params = default_params_by_mode()
    for k, v in c.items():
        if k in params:
            params[k] = v

    return env.Command(
        join(outdir, 'model_params.json'),
        [],
        f"echo '{json.dumps(params)}' > $TARGET")[0]


@nest.add_target_with_env(localenv, 'trained')
@ns.name_targets
def trained(env, outdir, c):
    return 'weights', 'diagnostics', env.Command(
        [join(outdir, 'best_weights.h5'), join(outdir, 'diagnostics.csv')],
        [c['model_params'], c['split']['training']],
        cluster_execution_string('tcr-vae train {sources} {targets}', localenv))


# @nest.add_target_with_env(localenv)
# def loss(env, outdir, c):
#     loss = join(outdir, 'loss.csv')
#     return env.Command(
#         loss,
#         [c['model_params'], c['trained']['weights'], c['split']['training'],
#             c['split']['validation']],
#         cluster_execution_string('tcr-vae loss {sources} {targets}'))[0]


@nest.add_target_with_env(localenv)
def training_pvae(env, outdir, c):
    pvae_call = f"tcr-vae pvae --limit-input-to {c['nseqs']}"
    return env.Command(
        join(outdir, common.strip_dirpath_extn(c['split']['training'])+'.head.pvae.csv'),
        [c['model_params'], c['trained']['weights'], c['split']['training']],
        cluster_execution_string(pvae_call + ' {sources} {targets}', localenv))[0]


@nest.add_target_with_env(localenv)
def validation_pvae(env, outdir, c):
    return env.Command(
        join(outdir, common.strip_dirpath_extn(c['validation_head'])+'.pvae.csv'),
        [c['model_params'], c['trained']['weights'], c['validation_head']],
        cluster_execution_string('tcr-vae pvae {sources} {targets}', localenv))[0]


@nest.add_target_with_env(localenv)
def vae_generated(env, outdir, c):
    return env.Command(
        join(outdir, 'vae-generated.csv'),
        [c['model_params'], c['trained']['weights']],
        f"tcr-vae generate --nseqs {c['nseqs']} $SOURCES $TARGET")[0]


@nest.add_target_with_env(localenv)
def vae_generated_pgen(env, outdir, c):
    return env.Command(
        join(outdir, 'vae-generated.pgen.tsv'),
        c['vae_generated'],
        cluster_execution_string('adaptive-pgen.sh {sources} {targets}', localenv, 0))[0]


@nest.add_target_with_env(localenv)
def vae_generated_ppost(env, outdir, c):
    return env.Command(
        join(outdir, 'vae-generated.ppost.csv'),
        [c['q_csv'], c['vae_generated_pgen']],
        'python3 thymic_Q.py ppost $SOURCES $TARGET')[0]


@nest.add_target_with_env(localenv)
def vae_generated_pvae(env, outdir, c):
    return env.Command(
        join(outdir, 'vae-generated.pvae.csv'),
        [c['model_params'], c['trained']['weights'], c['vae_generated']],
        cluster_execution_string('tcr-vae pvae {sources} {targets}', localenv))[0]


@nest.add_target_with_env(localenv)
def olga_generated_pvae(env, outdir, c):
    return env.Command(
        join(outdir, 'olga-generated.pvae.csv'),
        [c['model_params'], c['trained']['weights'], c['olga_generated']],
        cluster_execution_string('tcr-vae pvae {sources} {targets}', localenv))[0]


# Nest: evaluation on the test sets.
nest.add('test_set', lambda c: c['test_set_agg'], label_func=common.strip_dirpath_extn)
summarized_agg_names.append('test_set')


# These next two things are handy for outputting where we are in the nest.
idx_name = ';'.join(summarized_agg_names)


def get_idx(c):
    """
    Return a semicolon-separated list of where we are in the nest.
    """
    return ';'.join([str(c[k]) for k in summarized_agg_names])


# @nest.add_target_with_env(localenv)
# def test_per_sequence_loss(env, outdir, c):
#     return env.Command(
#         join(outdir, common.strip_dirpath_extn(c['test_set'])+'.per-seq-loss.csv'),
#         [c['model_params'], c['trained'], c['test_set']],
#         'tcr-vae per-seq-loss $SOURCES $TARGET')[0]


@nest.add_target_with_env(localenv)
def test_pvae(env, outdir, c):
    return env.Command(
        join(outdir, 'test.pvae.csv'),
        [c['model_params'], c['trained']['weights'], c['test_set']],
        cluster_execution_string('tcr-vae pvae {sources} {targets}', localenv))[0]


@nest.add_target_with_env(localenv)
def merged_ps(env, outdir, c):
    c['test_ppost'] = c['test_set_info_agg'][(c['test_set'], 'test_ppost')]
    return env.Command(
        join(outdir, 'merged_ps.csv'),
        [c['test_set'], c['test_pvae'], c['test_ppost']],
        f'python3 util.py merge-ps --idx "{get_idx(c)}" --idx-name "{idx_name}" $SOURCES $TARGET')[0]


@nest.add_target_with_env(localenv)
def merged_plus_sumrep(env, outdir, c):
    merged = join(outdir, 'merged_ps.sumrep.csv')
    c['merged_agg'].append(merged)
    return env.Command(
        merged,
        c['merged_ps'],
        'R/add_cdr3_summaries.R $SOURCE $TARGET')[0]


@nest.add_target_with_env(localenv)
def vae_sumrep_divergences(env, outdir, c):
    return env.Command(
        join(outdir, 'vae.sumdiv.csv'),
        [c['test_set'], c['vae_generated']],
        'R/rep_divergences.R $SOURCES $TARGET')[0]


# @nest.add_target_with_env(localenv)
# def regress_loss(env, outdir, c):
#     loss_regression = join(outdir, 'loss_regression.csv'),
#     c['loss_regression_agg'].append(loss_regression)
#     return env.Command(
#         loss_regression,
#         [c['test_pvae'], c['test_per_sequence_loss']],
#         f'R/regress_loss.R --idx "{get_idx(c)}" --idx-name "{idx_name}" $SOURCES $TARGET')[0]


# @nest.add_target_with_env(localenv)
# def auc_ppost(env, outdir, c):
#     c['test_ppost'] = c['test_set_info_agg'][(c['test_set'], 'test_ppost')]
#     return env.Command(
#         join(outdir, 'ppost-auc.csv'),
#         [c['test_ppost'], c['vae_generated_ppost']],
#         'R/auc.R $SOURCES $TARGET')[0]


# @nest.add_target_with_env(localenv)
# def auc_pvae(env, outdir, c):
#     return env.Command(
#         join(outdir, 'pvae-auc.csv'),
#         [c['test_pvae'], c['olga_generated_pvae']],
#         'R/auc.R --pvae $SOURCES $TARGET')[0]
#

@nest.add_target_with_env(localenv)
def summarized(env, outdir, c):
    summarized = join(outdir, 'summarized.csv')
    # Adding things to the summarized_agg aggregate.
    c['summarized_agg'].append(summarized)
    to_pass = ['training_pvae', 'validation_pvae', 'test_pvae',
               'vae_sumrep_divergences']
    colnames = ','.join(to_pass)
    idx = get_idx(c)
    return env.Command(
        summarized,
        [c[k] for k in to_pass],
        f'python3 util.py summarize --out $TARGET --idx "{idx}" --idx-name "{idx_name}" --colnames {colnames} $SOURCES'
    )[0]


# Now we pop out past model, so that we can do some OLGA stuff that doesn't depend on the VAE model.
nest.pop('model')

# Nest: loop back over the test sets so we can calculate things for OLGA.
nest.add('test_set', lambda c: c['test_set_agg'], label_func=common.strip_dirpath_extn)


@nest.add_target_with_env(localenv)
def olga_sumrep_divergences(env, outdir, c):
    return env.Command(
        join(outdir, 'olga.sumdiv.csv'),
        [c['test_set'], c['olga_generated']],
        'R/rep_divergences.R $SOURCES $TARGET')[0]


@nest.add_target_with_env(localenv)
def olga_summarized(env, outdir, c):
    summarized = join(outdir, 'olga-summarized.csv')
    c['summarized_agg'].append(summarized)
    c['test_ppost'] = c['test_set_info_agg'][(c['test_set'], 'test_ppost')]
    to_pass = ['training_ppost', 'validation_ppost', 'test_ppost', 'olga_sumrep_divergences']
    colnames = ','.join(to_pass)
    # Here we populate the OLGA indices with some things that are not relevant, such as VAE model parameters.
    # This repeats unnecessarily, but it makes downstream plotting easier because we can treat all the models the same.
    c_copy = dict(c)
    c_copy['model'] = 'olga'
    idx = get_idx(c_copy)
    return env.Command(
        summarized,
        [c[k] for k in to_pass],
        f'python3 util.py summarize --out $TARGET --idx "{idx}" --idx-name "{idx_name}" --colnames {colnames} $SOURCES'
    )[0]


@nest.add_target_with_env(localenv)
def train_sumrep_divergences(env, outdir, c):
    return env.Command(
        join(outdir, 'train.sumdiv.csv'),
        [c['test_set'], c['validation_head']],
        'R/rep_divergences.R $SOURCES $TARGET')[0]


@nest.add_target_with_env(localenv)
def train_summarized(env, outdir, c):
    summarized = join(outdir, 'train-summarized.csv')
    c['summarized_agg'].append(summarized)
    # Here we populate the train indices with some things that are not relevant, such as VAE model parameters.
    # This repeats unnecessarily, but it makes downstream plotting easier because we can treat all the models the same.
    c_copy = dict(c)
    c_copy['model'] = 'train'
    idx = get_idx(c_copy)
    return env.Command(
        summarized,
        c['train_sumrep_divergences'],
        f'python3 util.py summarize --out $TARGET --idx "{idx}" --idx-name "{idx_name}" '
        f'--colnames train_sumrep_divergences $SOURCE'
    )[0]


# Pop all the way back to the very beginning for a complete summary.
nest.pop(summarized_agg_names[0])


@nest.add_target_with_env(localenv)
def summarized_agg_target(env, outdir, c):
    return env.Command(
        join(outdir, 'summarized.agg.csv'),
        c['summarized_agg'],
        'python3 util.py fancystack --out $TARGET $SOURCES')[0]


@nest.add_target_with_env(localenv)
def merged_agg_target(env, outdir, c):
    return env.Command(
        join(outdir, 'merged.agg.csv'),
        c['merged_agg'],
        'python3 util.py fancystack --out $TARGET $SOURCES')[0]


@nest.add_target_with_env(localenv)
def merged_agg_target_bz2(env, outdir, c):
    return env.Command(
        str(c['merged_agg_target'])+'.bz2',
        c['merged_agg_target'],
        'bzip2 $SOURCE')[0]


# @nest.add_target_with_env(localenv)
# def loss_regression_agg_target(env, outdir, c):
#     return env.Command(
#         join(outdir, 'loss_regression.agg.csv'),
#         c['loss_regression_agg'],
#         'python3 util.py stackrows --out $TARGET $SOURCES')[0]
